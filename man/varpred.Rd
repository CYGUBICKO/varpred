% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/varpred.R
\docType{package}
\name{varpred}
\alias{varpred}
\title{Prediction and effect estimates}
\usage{
varpred(
  mod,
  focal_predictors,
  x.var = NULL,
  type = c("response", "link"),
  isolate = TRUE,
  isolate.value = NULL,
  level = 0.95,
  steps = 100,
  at = list(),
  dfspec = 100,
  true.beta = NULL,
  vcov. = NULL,
  internal = FALSE,
  input_vars = FALSE,
  avefun = mean,
  offset = NULL,
  bias.adjust = c("none", "taylor", "observed"),
  sigma = NULL,
  include.re = FALSE,
  modelname = NULL,
  returnall = FALSE
)
}
\arguments{
\item{mod}{fitted model object. See details for supported class of models.}

\item{focal_predictors}{a character vector of one or more predictors. For a model with an interaction, the interacting variables are specified as a vector,  for example \code{~x1*x2} will be \code{c("x1", "x2")}. If no interactions are present in the model, specifying a vector of variables compares predictions between them.}

\item{x.var}{a character specifying the predictor to define the x variable (horizontal axis on the plot). The default is \code{NULL}, of which the first predictor in \code{focal_predictors} is used. Ignored if there is a single \code{focal_predictors}.}

\item{type}{a character specifying the desired prediction. \code{type = "response"} applies inverse transformation of the link functions, if exists. \code{type = "link"} requests the results as a linear predictor.}

\item{isolate}{logical. If \code{TRUE} (default), computes effect estimates otherwise it computes prediction estimates. See details.}

\item{isolate.value}{numeric (default \code{isolate.value = NULL}). If \code{isolate = TRUE}, otherwise ignored, is the value to use as the \emph{anchor}. The default value, computed internally, is the average of the linear predictor(s) corresponding to focal predictors.}

\item{level}{desired confidence interval for computing the confidence intervals. Default is \code{0.95}.}

\item{steps}{number of points to evaluate numerical predictors in \code{focal_predictors}. The default is \code{100}. Increase for smooth curves. Unique levels of \code{focal_predictors} are used in the case categorical predictors.}

\item{at}{default \code{NULL}. Otherwise, is a named \code{list} specifying points to evaluate \code{focal_predictors}. The names in the list should match the names used in \code{focal_predictors}. If \code{NULL}, the levels are internally generated using quantile, see \code{\link[stats]{quantile}}.}

\item{dfspec}{default \code{100}. Specified degrees of freedom for a model which do not return \code{df}. This is used in computation of confidence intervals.}

\item{true.beta}{default \code{NULL}. If specified, used as model coefficient estimates and should be in the same order as the vector of coefficients from the model object. Useful when comparing model estimates to the "truth" (simulation values).}

\item{vcov.}{a function or a matrix. If a function, it is used to compute the variance-covariance matrix of the model coefficients. The function should take the model as it's first (or maybe only) argument. A matrix of variance-covariance matrix of the estimated coefficient can also be used. Otherwise \code{vcov(mod)} is used internally. Customized \code{vcov.} can be used to generate effect estimates if the columns corresponding to the non-focal predictors are all zero. However, with this approach, the predictors should be properly scaled. See examples.}

\item{internal}{logical. If \code{TRUE}, the entries of the non-focal predictor (see x.var) in the variance-covariance matrix are internally zeroed-out using \code{\link[varpred]{zero_vcov}}. Default is \code{FALSE}.}

\item{input_vars}{logical. If \code{TRUE}, package input variables are averaged otherwise linear predictor variables. See examples in vignette.}

\item{avefun}{the averaging scheme (function) to be used in generating reference point for non-focal predictors. Default is \code{mean}.}

\item{offset}{a function or a value.}

\item{bias.adjust}{specifies the bias correction method. If "none" (default), no bias correction method is applied; if "taylor", second-order Taylor approximation is used; if "observed", all the values of non-focal predictors are used. See details and examples.}

\item{sigma}{standard deviation used if \code{bias.adjust="taylor"}. If \code{NULL} (default), \code{\link[stats]{sigma}} or \code{VarCorr} is used.}

\item{include.re}{logical. Default is \code{FALSE}. If \code{TRUE}, the random effects components of mixed models is included.}

\item{modelname}{character string naming \code{varpred} objects. Useful when comparing several objects.}

\item{returnall}{logical. If \code{TRUE}, all other named computed quantities are also returned.}
}
\description{
Computes central estimates, prediction and effect estimates for a particular focal predictor.
}
\details{
The central estimates (often called effect or prediction) describe how the fitted model responds to the changes in the focal predictor. The other associated quantities are the prediction and effect estimates:
\itemize{
\item \strong{prediction estimates}: incorporate all the sources of uncertainty in the model. Important if our goal is to do prediction.
\item \strong{effect estimates}: incorporate uncertainty due to focal predictor only. Focus on visualizing the \emph{effect} of the focal predictor.
}

The default approaches to compute these quantities involves averaging the non-focal linear predictors (columns of \code{\link[stats]{model.matrix}} corresponding to non-focal predictors) -- \emph{mean-based} approach. An alternative is the \emph{observed-value-based} approach which computes the estimates over the entire population of the non-focal predictors and then averages them over the levels of the focal predictors. The later approach is more appropriate for a model involving non-linear link function with non-focal predictors and/or random effects. See \code{vignette("vapred_intro", package="varpred")}) for more details.

The current version supports:
\itemize{
\item lm and glm
\item lme4
\item glmmTMB
\item rstanarm
}

objects.
}
\examples{
# Set theme for ggplot. Comment out if not needed
library(ggplot2)
varpredtheme()
set.seed(911)
# Simulate binary outcome data with two predictors
steps <- 500
N <- 100
b0 <- 2
b_age <- -1.5
b_income <- 1.8
min_age <- 18
age <- min_age + rnorm(N, 0, 1)
min_income <- 15
income <- min_income + rnorm(N, 0, 1)
eta <- b0 + age*b_age + income*b_income
status <- rbinom(N, 1, plogis(eta))
df <- data.frame(status, age, income)

# Fit model
mod <- glm(status ~ age + income, df, family=binomial())

# Effect plots
## Mean-based
ef_mean <- varpred(mod, "age", steps=steps, bias.adjust="none", modelname="mean-based")
## Observed-value-based
ef_observed <- varpred(mod, "age", steps=steps, bias.adjust="observed", modelname="observed-value")
## Combine all the effect estimates
ef <- combinevarpred(list(ef_mean, ef_observed))
print(plot(ef)
	+ scale_color_brewer(palette = "Dark2")
)

# Prediction plots
## Mean-based
pred_mean <- varpred(mod, "age", isolate=FALSE
	, steps=steps, bias.adjust="none"
	, modelname="mean-based"
)
## Observed-value-based
pred_observed <- varpred(mod, "age", isolate=FALSE
	, steps=steps, bias.adjust="observed"
	, modelname="observed-value"
)
## Combine all the prediction estimates
### With plotit=TRUE no need to plot
pred <- combinevarpred(list(pred_mean, pred_observed), plotit=TRUE)
print(pred
	+ scale_color_brewer(palette = "Dark2")
)

}
\seealso{
\code{\link[varpred]{plot.varpred}}
}
